#!/usr/bin/env python3
"""
PFA and derivative measure (LP16 ¬ß6) on a sub-Alfv√©nic turbulence cube.

- Two geometries:
  1) Separated regions (external Faraday screen): synchrotron emission and Faraday rotation are spatially distinct.
  2) Mixed volume: emission and Faraday rotation co-exist along the LOS.

- Outputs (LP16-like presentation):
  ‚Ä¢ ‚ü®|P(Œª)|^2‚ü© vs Œª^2  (PFA)
  ‚Ä¢ ‚ü®|dP/dŒª^2|^2‚ü© vs Œª^2 (derivative measure from LP16 ¬ß6)

The formulas used:
  P(X, Œª) = ‚à´ dz P_i(X,z) exp[ 2 i Œª^2 Œ¶(X,z) ],  with  Œ¶(X,z) = ‚à´_0^z œÜ(X,z') dz',
  where œÜ ‚àù n_e B_‚à• and P_i ‚àù (B_x + i B_y)^{(Œ≥+1)/2}.  In the external-screen case,
  P(X, Œª) = P_emit(X) exp[ 2 i Œª^2 Œ¶_screen(X) ].

  Derivative measure:
  dP/dŒª^2 = ‚à´ dz ( 2 i P_i Œ¶ ) exp[ 2 i Œª^2 Œ¶ ]   (mixed),
  dP/dŒª^2 = ( 2 i Œ¶_screen ) P  (separated).

These follow LP16 eqs. (158‚Äì163) for the derivative correlation and the definitions
of P; the PFA itself is the R=0 case of the two-point correlation (variance). See LP16
for regimes and scalings.

NOTE: This script focuses on getting LP16-like curves from your cube; absolute units
are arbitrary unless faraday_const is calibrated.
"""
from __future__ import annotations
import h5py
import numpy as np
import matplotlib.pyplot as plt
from dataclasses import dataclass
from typing import Optional, Tuple, Iterable
from multiprocessing import Pool, cpu_count
import functools
from numpy.random import default_rng

# -----------------------------
# User I/O settings
# -----------------------------
H5_PATH = r"..\faradays_angles_stats\lp_structure_tests\ms01ma08.mhd_w.00300.vtk.h5"
BX_KEY, BY_KEY, BZ_KEY = "i_mag_field", "j_mag_field", "k_mag_field"
NE_KEY = "gas_density"

# -----------------------------
# Config and helpers
# -----------------------------
@dataclass
class PFAConfig:
    lam_grid: Iterable[float] = tuple(np.sqrt(np.geomspace(0.05**2, 10.0**2, 500)))
    lam_mark: float = 1.0
    gamma: float = 2.0            # electron index ‚Üí P_i ‚àù (B_‚ä•)^{(Œ≥+1)/2}
    faraday_const: float = 0.81    # arbitrary units here; relative scaling only
    los_axis: int = 0              # integrate along this axis
    voxel_depth: float = 1.0       # Œîz in code units


class FieldKeys:
    Bx: str = BX_KEY
    By: str = BY_KEY
    Bz: str = BZ_KEY
    ne: str = NE_KEY


def load_fields(h5_path: str, keys: FieldKeys):
    with h5py.File(h5_path, "r") as f:
        Bx = np.array(f[keys.Bx])
        By = np.array(f[keys.By])
        Bz = np.array(f[keys.Bz])
        ne = np.array(f[keys.ne])
    return Bx, By, Bz, ne


def polarized_emissivity(Bx: np.ndarray, By: np.ndarray, gamma: float = 2.0) -> np.ndarray:
    """
    LP16-correct polarized emissivity: P_i = (Bx + i*By)^2 * |B_perp|^{(gamma-3)/2}
    For gamma=2: P_i = (Bx + i*By)^2 / |B_perp|
    """
    B2 = Bx**2 + By**2
    eps = np.finfo(B2.dtype).tiny
    amp = np.power(np.maximum(B2, eps), 0.5*(gamma - 3.0))
    return ((Bx + 1j*By)**2 * amp).astype(np.complex128)


def faraday_density(ne: np.ndarray, Bpar: np.ndarray, C: float = 0.81) -> np.ndarray:
    return C * ne * Bpar


def _move_los(arr: np.ndarray, axis: int) -> np.ndarray:
    return np.moveaxis(arr, axis, 0)


def psa_of_map(P_map: np.ndarray, ring_bins: int = 48, pad: int = 1,
               apodize: bool = True, k_min: float = 6.0,
               min_counts_per_ring: int = 10,
               beam_sigma_px: float = 0.0) -> Tuple[np.ndarray, np.ndarray]:
    """
    Compute azimuthally-averaged power spectrum (PSA) of a 2D polarization map.
    This is the spatial statistic discussed in LP16 ¬ß6.
    
    Improved version with:
    - Geometric (log) binning for better slope fitting
    - Zero-padding to reduce aliasing
    - Hann windowing to reduce edge effects
    - Careful k-range selection (avoids undersampled low-k and noisy high-k)
    
    Args:
        P_map: 2D complex polarization map
        ring_bins: number of logarithmic k-bins
        pad: zero-padding factor (1=no padding, 2=double size)
        apodize: apply 2D Hann window
        k_min: minimum k to include (avoid undersampled modes)
    
    Returns:
        k_centers: wavenumber bins (geometric mean)
        E_k: ring-averaged power spectrum
    """
    # Detrend (always)
    Y = P_map - P_map.mean()
    
    # Apply 2D Hann window
    if apodize:
        wy = np.hanning(Y.shape[0])
        wx = np.hanning(Y.shape[1])
        Y = Y * np.outer(wy, wx)
    
    # Zero-pad
    if pad > 1:
        Yp = np.zeros((pad * Y.shape[0], pad * Y.shape[1]), dtype=complex)
        Yp[:Y.shape[0], :Y.shape[1]] = Y
        Y = Yp
    
    # 2D FFT (optionally apply Gaussian telescope beam in Fourier space)
    F = np.fft.fft2(Y)
    if beam_sigma_px > 0:
        ky = np.fft.fftfreq(Y.shape[0]) * Y.shape[0]
        kx = np.fft.fftfreq(Y.shape[1]) * Y.shape[1]
        KX, KY = np.meshgrid(kx, ky)
        G = np.exp(-0.5 * beam_sigma_px**2 * (KX**2 + KY**2))
        F *= G
    F = np.fft.fftshift(F)
    P2 = (F * np.conj(F)).real
    
    # Wavenumber grids
    ky = np.fft.fftshift(np.fft.fftfreq(P2.shape[0])) * P2.shape[0]
    kx = np.fft.fftshift(np.fft.fftfreq(P2.shape[1])) * P2.shape[1]
    KX, KY = np.meshgrid(kx, ky)
    KR = np.hypot(KX, KY).ravel()
    
    # Geometric binning (better for power-law fitting)
    k_max = min(P2.shape) / 3.5  # a bit lower than Nyquist/4 on 256 grids
    bins = np.geomspace(max(1.0, k_min), k_max, ring_bins + 1)
    lab = np.digitize(KR, bins) - 1
    
    # Ring-average with geometric mean for k
    # ring population counts (avoid under-sampled bins)
    counts = np.array([(lab == i).sum() for i in range(ring_bins)])
    Ek = np.array([P2.ravel()[lab == i].mean() if counts[i] >= min_counts_per_ring else np.nan
                   for i in range(ring_bins)])
    kcen = np.sqrt(bins[:-1] * bins[1:])  # geometric mean
    
    # Keep only well-sampled bins
    msk = np.isfinite(Ek) & (kcen >= k_min) & (kcen <= k_max)

    # Fallback: integer-radius rings if too few bins survived
    if msk.sum() < 10:
        ky = np.fft.fftshift(np.fft.fftfreq(P2.shape[0])) * P2.shape[0]
        kx = np.fft.fftshift(np.fft.fftfreq(P2.shape[1])) * P2.shape[1]
        KX, KY = np.meshgrid(kx, ky)
        KR = np.hypot(KX, KY)
        kr = np.floor(KR + 0.5).astype(int)
        kmax_int = int(min(P2.shape) / 2)  # up to Nyquist
        weights = P2.ravel()
        idx = kr.ravel()
        Ek_int = np.bincount(idx, weights=weights, minlength=kmax_int + 1)
        Nk_int = np.bincount(idx, minlength=kmax_int + 1)
        with np.errstate(invalid="ignore", divide="ignore"):
            Ek_int = Ek_int / np.maximum(Nk_int, 1)
        kvec = np.arange(kmax_int + 1)
        msk2 = (kvec >= int(k_min)) & (kvec <= int(k_max)) & (Nk_int > 25)
        # drop very low-k and very high-k
        kvec = kvec[msk2]
        Ek_int = Ek_int[msk2]
        return kvec, Ek_int

    return kcen[msk], Ek[msk]


def compute_faraday_regime(phi: np.ndarray, verbose: bool = True) -> dict:
    """
    Diagnose the Faraday rotation regime and estimate key length scales.
    
    Returns dict with:
        - phi_mean: mean Faraday density
        - phi_std: std Faraday density  
        - regime: "random-dominated" or "mean-dominated"
        - ratio: |phi_mean| / phi_std
        - r_i: crude RM correlation length estimate (in pixels)
    """
    phi_mean = np.mean(phi)
    phi_std = np.std(phi)
    ratio = abs(phi_mean) / phi_std if phi_std > 0 else np.inf
    regime = "random-dominated" if ratio < 1 else "mean-dominated"
    
    # Crude correlation length estimate (autocorrelation scale)
    # Use central slice for speed
    mid = phi.shape[0] // 2
    phi_slice = phi[mid, :, :]
    phi_slice = phi_slice - phi_slice.mean()
    
    # 2D autocorrelation via FFT
    F = np.fft.fft2(phi_slice)
    acf = np.fft.ifft2(F * np.conj(F)).real
    acf = np.fft.fftshift(acf)
    acf = acf / acf.max()
    
    # Find where ACF drops to 1/e
    center = np.array(acf.shape) // 2
    y, x = np.ogrid[:acf.shape[0], :acf.shape[1]]
    r = np.hypot(y - center[0], x - center[1])
    
    # Radial profile
    r_bins = np.arange(0, min(acf.shape) // 2, 1)
    acf_radial = []
    for i in range(len(r_bins) - 1):
        mask = (r >= r_bins[i]) & (r < r_bins[i + 1])
        if np.any(mask):
            acf_radial.append(acf[mask].mean())
        else:
            acf_radial.append(0)
    
    acf_radial = np.array(acf_radial)
    # Find where it drops below 1/e
    idx = np.where(acf_radial < 1.0 / np.e)[0]
    r_i = r_bins[idx[0]] if len(idx) > 0 else 5.0  # default fallback
    
    result = {
        'phi_mean': phi_mean,
        'phi_std': phi_std,
        'ratio': ratio,
        'regime': regime,
        'r_i': float(r_i)
    }
    
    if verbose:
        print("\nFaraday regime diagnostic:")
        print(f"  Mean œÜ: {phi_mean:.4f}")
        print(f"  Std œÜ:  {phi_std:.4f}")
        print(f"  |œÜÃÑ|/œÉ_œÜ: {ratio:.4f}")
        print(f"  ‚Üí {regime.upper()}")
        print(f"  RM correlation length r_i ‚âà {r_i:.1f} pixels")
    
    return result


def effective_faraday_depth(lam: float, phi_info: dict) -> float:
    """
    Compute effective Faraday depth L_eff ~ 1 / [Œª¬≤ max(|œÜÃÑ|, œÉ_œÜ)]
    
    This determines if we're in short-Œª (L_eff >> L) or long-Œª (L_eff << L) regime.
    """
    phi_scale = max(abs(phi_info['phi_mean']), phi_info['phi_std'])
    if phi_scale == 0:
        return np.inf
    return 1.0 / (lam**2 * phi_scale)


def choose_lambda_for_regime(phi_info: dict, LOS_depth: float, 
                             target_regime: str = "long") -> float:
    """
    Choose Œª to target specific regime.
    
    Args:
        phi_info: output from compute_faraday_regime
        LOS_depth: depth of LOS integration in pixels
        target_regime: "short" (weak rotation) or "long" (strong rotation)
    
    Returns:
        optimal Œª value
    """
    phi_scale = max(abs(phi_info['phi_mean']), phi_info['phi_std'])
    r_i = phi_info['r_i']
    
    if target_regime == "long":
        # target exactly L_eff‚âàr_i
        lam_opt = 1.0 / np.sqrt(phi_scale * (1.0 * r_i))
    else:  # "short"
        # Want L_eff ~ LOS_depth (weak rotation)
        # Œª ~ 1/‚àö(œÜ_scale * LOS_depth)
        lam_opt = 1.0 / np.sqrt(phi_scale * LOS_depth)
        # Nudge slightly smaller, but stay within regime; 0.8 is plenty
        lam_opt *= 0.8
    
    return lam_opt


def decorrelate_phi_along_z(phi: np.ndarray, los_axis: int, seed: int = 0, mode: str = "roll") -> np.ndarray:
    """Break Pi‚ÄìŒ¶ correlations by rolling œÜ along the LOS per sightline (preserves r_i)."""
    rng = default_rng(seed)
    phi_los = np.moveaxis(phi, los_axis, 0)
    out = np.empty_like(phi_los)
    for j in range(phi_los.shape[1]):
        for i in range(phi_los.shape[2]):
            col = phi_los[:, j, i]
            if mode == "roll":
                out[:, j, i] = np.roll(col, rng.integers(0, col.size))
            else:  # "shuffle" (discouraged - wrecks r_i)
                out[:, j, i] = rng.permutation(col)
    return np.moveaxis(out, 0, los_axis)

# -----------------------------
# P and dP/dŒª¬≤ maps for two geometries
# -----------------------------

def P_map_separated(Pi: np.ndarray, phi: np.ndarray, lam: float, cfg: PFAConfig,
                    emit_bounds: Optional[Tuple[int, int]] = None,
                    screen_bounds: Optional[Tuple[int, int]] = None) -> np.ndarray:
    """P for external screen: P = P_emit * exp(2 i Œª¬≤ Œ¶_screen)."""
    Pi_los = _move_los(Pi, cfg.los_axis)
    phi_los = _move_los(phi, cfg.los_axis)
    Nz = Pi_los.shape[0]
    e0 = emit_bounds or (0, max(1, int(0.9 * Nz)))
    s0 = screen_bounds or (max(0, e0[1]), Nz)
    P_emit = np.sum(Pi_los[e0[0]:e0[1], :, :], axis=0) * cfg.voxel_depth
    Phi_screen = np.sum(phi_los[s0[0]:s0[1], :, :], axis=0) * cfg.voxel_depth
    return P_emit * np.exp(2j * (lam**2) * Phi_screen)


def P_map_mixed(Pi: np.ndarray, phi: np.ndarray, lam: float, cfg: PFAConfig,
                bounds: Optional[Tuple[int, int]] = None) -> np.ndarray:
    Pi_los = _move_los(Pi, cfg.los_axis)
    phi_los = _move_los(phi, cfg.los_axis)
    Nz = Pi_los.shape[0]
    z0, z1 = bounds or (0, Nz)
    Phi_cum = np.cumsum(phi_los[z0:z1, :, :] * cfg.voxel_depth, axis=0)
    phase = np.exp(2j * (lam**2) * Phi_cum)
    return np.sum(Pi_los[z0:z1, :, :] * phase, axis=0) * cfg.voxel_depth


def dP_map_separated(Pi: np.ndarray, phi: np.ndarray, lam: float, cfg: PFAConfig,
                      emit_bounds: Optional[Tuple[int, int]] = None,
                      screen_bounds: Optional[Tuple[int, int]] = None) -> np.ndarray:
    """Derivative dP/d(Œª¬≤) for external screen: 2i Œ¶_screen * P_emit * e^{2i Œª¬≤ Œ¶_screen}."""
    P = P_map_separated(Pi, phi, lam, cfg, emit_bounds=emit_bounds, screen_bounds=screen_bounds)
    phi_los = _move_los(phi, cfg.los_axis)
    Nz = phi_los.shape[0]
    s0 = screen_bounds or (max(0, int(0.9 * Nz)), Nz)
    Phi_screen = np.sum(phi_los[s0[0]:s0[1], :, :], axis=0) * cfg.voxel_depth
    return 2j * Phi_screen * P


def dP_map_mixed(Pi: np.ndarray, phi: np.ndarray, lam: float, cfg: PFAConfig,
                  bounds: Optional[Tuple[int, int]] = None) -> np.ndarray:
    """Derivative dP/d(Œª¬≤) for mixed geometry: 2i ‚à´ Pi(z) Œ¶(z) e^{2i Œª¬≤ Œ¶(z)} dz."""
    Pi_los = _move_los(Pi, cfg.los_axis)
    phi_los = _move_los(phi, cfg.los_axis)
    Nz = Pi_los.shape[0]
    z0, z1 = bounds or (0, Nz)
    Phi_cum = np.cumsum(phi_los[z0:z1, :, :] * cfg.voxel_depth, axis=0)
    phase = np.exp(2j * (lam**2) * Phi_cum)
    contrib = 2j * (Pi_los[z0:z1, :, :] * Phi_cum) * phase
    return np.sum(contrib, axis=0) * cfg.voxel_depth

# -----------------------------
# Multiprocessing helper functions
# -----------------------------

def _compute_separated_single(args):
    """Helper function for parallel computation of single lambda value"""
    lam2, P_emit, Phi_screen = args
    P_map = P_emit * np.exp(2j * lam2 * Phi_screen)
    return np.mean(np.abs(P_map)**2)


def _compute_mixed_single(args):
    """Helper function for parallel computation of single lambda value"""
    lam2, Pi_los, Phi_cum, z0, z1, voxel_depth = args
    phase = np.exp(2j * lam2 * Phi_cum)
    contrib = Pi_los[z0:z1, :, :] * phase
    P_map = np.sum(contrib, axis=0) * voxel_depth
    return np.mean(np.abs(P_map)**2)


def _compute_derivative_separated_single(args):
    """Helper function for parallel computation of derivative measure for separated case"""
    lam2, P_emit, Phi_screen = args
    P_map = P_emit * np.exp(2j * lam2 * Phi_screen)
    dP_map = 2j * Phi_screen * P_map
    return np.mean(np.abs(dP_map)**2)


def _compute_derivative_mixed_single(args):
    """Helper function for parallel computation of derivative measure for mixed case"""
    lam2, Pi_los, Phi_cum, z0, z1, voxel_depth = args
    phase = np.exp(2j * lam2 * Phi_cum)
    contrib = 2j * (Pi_los[z0:z1, :, :] * Phi_cum) * phase
    dP_map = np.sum(contrib, axis=0) * voxel_depth
    return np.mean(np.abs(dP_map)**2)


# -----------------------------
# PFA curves (variance over sky vs Œª¬≤)
# -----------------------------

def pfa_curve_separated(Pi: np.ndarray, phi: np.ndarray, cfg: PFAConfig,
                        emit_bounds: Optional[Tuple[int, int]] = None,
                        screen_bounds: Optional[Tuple[int, int]] = None,
                        n_processes: int = 11):
    lam_grid = np.array(tuple(cfg.lam_grid), dtype=float)
    
    # Pre-compute P_emit and Phi_screen once
    Pi_los = _move_los(Pi, cfg.los_axis)
    phi_los = _move_los(phi, cfg.los_axis)
    Nz = Pi_los.shape[0]
    e0 = emit_bounds or (0, max(1, int(0.9 * Nz)))
    s0 = screen_bounds or (max(0, e0[1]), Nz)
    P_emit = np.sum(Pi_los[e0[0]:e0[1], :, :], axis=0) * cfg.voxel_depth
    Phi_screen = np.sum(phi_los[s0[0]:s0[1], :, :], axis=0) * cfg.voxel_depth
    
    # Parallel computation for all lambda values
    lam2_grid = lam_grid**2
    args_list = [(lam2, P_emit, Phi_screen) for lam2 in lam2_grid]
    
    with Pool(processes=n_processes) as pool:
        var = pool.map(_compute_separated_single, args_list)
    
    return lam2_grid, np.array(var)


def pfa_curve_mixed(Pi: np.ndarray, phi: np.ndarray, cfg: PFAConfig,
                    bounds: Optional[Tuple[int, int]] = None,
                    n_processes: int = 11):
    lam_grid = np.array(tuple(cfg.lam_grid), dtype=float)
    
    # Pre-compute arrays once for efficiency
    Pi_los = _move_los(Pi, cfg.los_axis)
    phi_los = _move_los(phi, cfg.los_axis)
    Nz = Pi_los.shape[0]
    z0, z1 = bounds or (0, Nz)
    
    # Pre-compute cumulative Faraday depth for efficiency
    Phi_cum = np.cumsum(phi_los[z0:z1, :, :] * cfg.voxel_depth, axis=0)
    
    # Parallel computation for all lambda values
    lam2_grid = lam_grid**2
    args_list = [(lam2, Pi_los, Phi_cum, z0, z1, cfg.voxel_depth) for lam2 in lam2_grid]
    
    with Pool(processes=n_processes) as pool:
        var = pool.map(_compute_mixed_single, args_list)
    
    return lam2_grid, np.array(var)


def pfa_curve_derivative_separated(Pi: np.ndarray, phi: np.ndarray, cfg: PFAConfig,
                                   emit_bounds: Optional[Tuple[int, int]] = None,
                                   screen_bounds: Optional[Tuple[int, int]] = None,
                                   n_processes: int = 11):
    lam_grid = np.array(tuple(cfg.lam_grid), dtype=float)
    
    # Pre-compute P_emit and Phi_screen once
    Pi_los = _move_los(Pi, cfg.los_axis)
    phi_los = _move_los(phi, cfg.los_axis)
    Nz = Pi_los.shape[0]
    e0 = emit_bounds or (0, max(1, int(0.9 * Nz)))
    s0 = screen_bounds or (max(0, e0[1]), Nz)
    P_emit = np.sum(Pi_los[e0[0]:e0[1], :, :], axis=0) * cfg.voxel_depth
    Phi_screen = np.sum(phi_los[s0[0]:s0[1], :, :], axis=0) * cfg.voxel_depth
    
    # Parallel computation for all lambda values
    lam2_grid = lam_grid**2
    args_list = [(lam2, P_emit, Phi_screen) for lam2 in lam2_grid]
    
    with Pool(processes=n_processes) as pool:
        var = pool.map(_compute_derivative_separated_single, args_list)
    
    return lam2_grid, np.array(var)


def pfa_curve_derivative_mixed(Pi: np.ndarray, phi: np.ndarray, cfg: PFAConfig,
                               bounds: Optional[Tuple[int, int]] = None,
                               n_processes: int = 11):
    lam_grid = np.array(tuple(cfg.lam_grid), dtype=float)
    
    # Pre-compute arrays once for efficiency
    Pi_los = _move_los(Pi, cfg.los_axis)
    phi_los = _move_los(phi, cfg.los_axis)
    Nz = Pi_los.shape[0]
    z0, z1 = bounds or (0, Nz)
    
    # Pre-compute cumulative Faraday depth for efficiency
    Phi_cum = np.cumsum(phi_los[z0:z1, :, :] * cfg.voxel_depth, axis=0)
    
    # Parallel computation for all lambda values
    lam2_grid = lam_grid**2
    args_list = [(lam2, Pi_los, Phi_cum, z0, z1, cfg.voxel_depth) for lam2 in lam2_grid]
    
    with Pool(processes=n_processes) as pool:
        var = pool.map(_compute_derivative_mixed_single, args_list)
    
    return lam2_grid, np.array(var)

# -----------------------------
# Plotting (LP16-style log‚Äìlog)
# -----------------------------

def fit_log_slope(x: np.ndarray, y: np.ndarray, xmin=None, xmax=None):
    mask = np.isfinite(x) & np.isfinite(y) & (x > 0) & (y > 0)
    if xmin is not None:
        mask &= (x >= xmin)
    if xmax is not None:
        mask &= (x <= xmax)
    xlog = np.log10(x[mask]); ylog = np.log10(y[mask])
    if len(xlog) < 3:
        return np.nan, np.nan
    A = np.vstack([xlog, np.ones_like(xlog)]).T
    m, a = np.linalg.lstsq(A, ylog, rcond=None)[0]
    return m, a


def fit_log_slope_window(k: np.ndarray, E: np.ndarray) -> Tuple[float, float, Tuple[float, float]]:
    """
    Auto-select clean inertial-range decade and fit power law.
    Avoids apodization roll-off regions.
    
    Returns:
        (slope, intercept, (k_min, k_max)) where log(E) = intercept + slope * log(k)
    """
    k = np.asarray(k); E = np.asarray(E)
    m = np.isfinite(k) & np.isfinite(E) & (k > 0) & (E > 0)
    k, E = k[m], E[m]
    if k.size < 12: 
        return np.nan, np.nan, (np.nan, np.nan)
    
    lk, lE = np.log10(k), np.log10(E)
    best = (np.inf, None, None, None)  # (curv, m, a, (lo,hi))
    
    for p in (30, 40, 50, 60, 70):
        kmid = 10**np.percentile(lk, p)
        for span in (2.0, 2.5, 3.0):  # ~0.8‚Äì1.2 decades
            lo, hi = kmid/10**(span/6), kmid*10**(span/6)
            s = (k >= lo) & (k <= hi)
            if s.sum() < 10: 
                continue
            sl = np.diff(lE[s])/np.diff(lk[s])
            curv = np.nanstd(sl)
            mfit, afit = np.polyfit(lk[s], lE[s], 1)
            if curv < best[0]:
                best = (curv, mfit, afit, (lo, hi))
    
    return best[1], best[2], best[3]


def plot_pfa(lam2: np.ndarray, var_list: list, labels: list, cfg: PFAConfig,
             title: str = "PFA: $\\langle|P|^2\\rangle$ vs $\\lambda^2$"):
    plt.figure(figsize=(6.5, 5.0))
    for var, lab in zip(var_list, labels):
        plt.loglog(lam2, var, label=lab)
    if var_list:
        m, a = fit_log_slope(lam2, var_list[0])
        if np.isfinite(m):
            mid = 10**np.nanmedian(np.log10(lam2))
            ymid = 10**(a + m * np.log10(mid))
            xline = np.array([mid/5, mid*5])
            yline = ymid * (xline/mid)**m
            plt.loglog(xline, yline, '--', linewidth=1, label=f"fit slope ‚âà {m:.2f}")
    lam_mark2 = cfg.lam_mark**2
    for var in var_list:
        vmark = np.interp(lam_mark2, lam2, var)
        plt.scatter([lam_mark2], [vmark], marker='o')
    plt.xlabel("$\\lambda^2$ (arb. units)")
    plt.ylabel("$\\langle |P|^2 \\rangle$ (arb. units)")
    plt.title(title)
    plt.grid(True, which='both', alpha=0.2)
    plt.legend()
    plt.tight_layout()


def plot_pfa_derivative(lam2: np.ndarray, var_list: list, labels: list, cfg: PFAConfig,
                        title: str = r"Derivative measure $\langle|dP/d\lambda^2|^2\rangle$ vs $\lambda^2$"):
    plt.figure(figsize=(6.5, 5.0))
    for var, lab in zip(var_list, labels):
        plt.loglog(lam2, var, label=lab)
    if var_list:
        m, a = fit_log_slope(lam2, var_list[0])
        if np.isfinite(m):
            mid = 10**np.nanmedian(np.log10(lam2))
            ymid = 10**(a + m * np.log10(mid))
            xline = np.array([mid/5, mid*5])
            yline = ymid * (xline/mid)**m
            plt.loglog(xline, yline, '--', linewidth=1, label=f"fit slope ‚âà {m:.2f}")
    lam_mark2 = cfg.lam_mark**2
    for var in var_list:
        vmark = np.interp(lam_mark2, lam2, var)
        plt.scatter([lam_mark2], [vmark], marker='o')
    plt.xlabel(r"$\lambda^2$ (arb. units)")
    plt.ylabel(r"$\langle |dP/d\lambda^2|^2 \rangle$ (arb. units)")
    plt.title(title)
    plt.grid(True, which='both', alpha=0.2)
    plt.legend()
    plt.tight_layout()


def plot_combined_pfa_and_derivative(lam2_pfa: np.ndarray, var_pfa_list: list, 
                                    lam2_der: np.ndarray, var_der_list: list,
                                    labels: list, cfg: PFAConfig):
    """Plot both PFA and derivative measures on the same figure with subplots"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))
    
    # PFA plot
    for var, lab in zip(var_pfa_list, labels):
        ax1.loglog(lam2_pfa, var, label=lab)
    if var_pfa_list:
        m, a = fit_log_slope(lam2_pfa, var_pfa_list[0])
        if np.isfinite(m):
            mid = 10**np.nanmedian(np.log10(lam2_pfa))
            ymid = 10**(a + m * np.log10(mid))
            xline = np.array([mid/5, mid*5])
            yline = ymid * (xline/mid)**m
            ax1.loglog(xline, yline, '--', linewidth=1, label=f"fit slope ‚âà {m:.2f}")
    lam_mark2 = cfg.lam_mark**2
    for var in var_pfa_list:
        vmark = np.interp(lam_mark2, lam2_pfa, var)
        ax1.scatter([lam_mark2], [vmark], marker='o')
    ax1.set_xlabel("$\\lambda^2$ (arb. units)")
    ax1.set_ylabel("$\\langle |P|^2 \\rangle$ (arb. units)")
    ax1.set_title("PFA: $\\langle|P|^2\\rangle$ vs $\\lambda^2$")
    ax1.grid(True, which='both', alpha=0.2)
    ax1.legend()
    
    # Derivative plot
    for var, lab in zip(var_der_list, labels):
        ax2.loglog(lam2_der, var, label=lab)
    if var_der_list:
        m, a = fit_log_slope(lam2_der, var_der_list[0])
        if np.isfinite(m):
            mid = 10**np.nanmedian(np.log10(lam2_der))
            ymid = 10**(a + m * np.log10(mid))
            xline = np.array([mid/5, mid*5])
            yline = ymid * (xline/mid)**m
            ax2.loglog(xline, yline, '--', linewidth=1, label=f"fit slope ‚âà {m:.2f}")
    for var in var_der_list:
        vmark = np.interp(lam_mark2, lam2_der, var)
        ax2.scatter([lam_mark2], [vmark], marker='o')
    ax2.set_xlabel(r"$\lambda^2$ (arb. units)")
    ax2.set_ylabel(r"$\langle |dP/d\lambda^2|^2 \rangle$ (arb. units)")
    ax2.set_title(r"Derivative measure $\langle|dP/d\lambda^2|^2\rangle$ vs $\lambda^2$")
    ax2.grid(True, which='both', alpha=0.2)
    ax2.legend()
    
    plt.tight_layout()


def plot_spatial_psa_comparison(k_P: np.ndarray, Ek_P: np.ndarray, 
                                k_dP: np.ndarray, Ek_dP: np.ndarray,
                                lam: float, case: str = "Mixed"):
    """
    Plot spatial power spectra (PSA) of P and dP/dŒª¬≤ to show the difference
    in spatial statistics that LP16 ¬ß6 discusses.
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))
    
    # Use improved auto-fit window function
    m_P, a_P, (k_min_fit, k_max_fit) = fit_log_slope_window(k_P, Ek_P)
    
    # PSA of P
    ax1.loglog(k_P, Ek_P, 'o-', markersize=3, label=f'PSA of $P(\\lambda={lam:.1f})$')
    if np.isfinite(m_P):
        k_fit = np.array([k_min_fit, k_max_fit])
        E_fit = 10**(a_P + m_P * np.log10(k_fit))
        ax1.loglog(k_fit, E_fit, '--', linewidth=2, color='red', 
                  label=f'slope = {m_P:.2f}')
        # Mark the fitting range
        ax1.axvspan(k_min_fit, k_max_fit, alpha=0.1, color='red', label='fit range')
    ax1.set_xlabel('$k$ (wavenumber)')
    ax1.set_ylabel('$E(k)$')
    ax1.set_title(f'Spatial PSA of $P$ ({case}, $\\lambda={lam:.1f}$)')
    ax1.grid(True, which='both', alpha=0.2)
    ax1.legend()
    
    # PSA of dP/dŒª¬≤
    m_dP, a_dP, (k_min_dP, k_max_dP) = fit_log_slope_window(k_dP, Ek_dP)
    ax2.loglog(k_dP, Ek_dP, 's-', markersize=3, color='darkorange',
              label=f'PSA of $dP/d\\lambda^2$ ($\\lambda={lam:.1f}$)')
    if np.isfinite(m_dP):
        k_fit = np.array([k_min_dP, k_max_dP])
        E_fit = 10**(a_dP + m_dP * np.log10(k_fit))
        ax2.loglog(k_fit, E_fit, '--', linewidth=2, color='red',
                  label=f'slope = {m_dP:.2f}')
        # Mark the fitting range
        ax2.axvspan(k_min_dP, k_max_dP, alpha=0.1, color='red', label='fit range')
    ax2.set_xlabel('$k$ (wavenumber)')
    ax2.set_ylabel('$E(k)$')
    ax2.set_title(f'Spatial PSA of $dP/d\\lambda^2$ ({case}, $\\lambda={lam:.1f}$)')
    ax2.grid(True, which='both', alpha=0.2)
    ax2.legend()
    
    plt.tight_layout()
    
    # Print slopes for comparison
    print(f"\n{case} case at Œª={lam:.1f}:")
    print(f"  PSA of P slope: {m_P:.2f} (fitted k‚àà[15,80])")
    print(f"  PSA of dP/dŒª¬≤ slope: {m_dP:.2f} (fitted k‚àà[15,80])")
    print(f"  ‚Üí Difference: {abs(m_dP - m_P):.2f}")
    print("  (LP16 predicts derivative PSA better recovers turbulence slope m)")
    
    return m_P, m_dP


def plot_derivative_slope_analysis(lam2_sep: np.ndarray, dvar_sep: np.ndarray,
                                  lam2_mix: np.ndarray, dvar_mix: np.ndarray,
                                  cfg: PFAConfig):
    """
    Plot and analyze derivative measure slopes in the range Œª¬≤ ‚àà [3√ó10‚Åª¬π, 10¬π].
    This focuses on the derivative measure behavior in the specified Œª¬≤ range.
    """
    # Define the Œª¬≤ range for analysis
    lam2_min = 0.3  # 3√ó10‚Åª¬π
    lam2_max = 10.0  # 10¬π
    
    # Create figure
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))
    
    # Plot derivative measure for separated case
    ax1.loglog(lam2_sep, dvar_sep, 'o-', markersize=4, label='Separated (screen)')
    
    # Fit slope in the specified range
    m_sep, a_sep = fit_log_slope(lam2_sep, dvar_sep, xmin=lam2_min, xmax=lam2_max)
    if np.isfinite(m_sep):
        lam2_fit = np.array([lam2_min, lam2_max])
        dvar_fit = 10**(a_sep + m_sep * np.log10(lam2_fit))
        ax1.loglog(lam2_fit, dvar_fit, '--', linewidth=2, color='red',
                  label=f'slope = {m_sep:.2f} (Œª¬≤‚àà[0.3,10])')
        # Mark the fitting range
        ax1.axvspan(lam2_min, lam2_max, alpha=0.1, color='red', label='fit range')
    
    ax1.set_xlabel('$\\lambda^2$ (arb. units)')
    ax1.set_ylabel('$\\langle |dP/d\\lambda^2|^2 \\rangle$ (arb. units)')
    ax1.set_title('Derivative Measure: Separated Case')
    ax1.grid(True, which='both', alpha=0.2)
    ax1.legend()
    
    # Plot derivative measure for mixed case
    ax2.loglog(lam2_mix, dvar_mix, 's-', markersize=4, color='darkorange', 
              label='Mixed (in-situ)')
    
    # Fit slope in the specified range
    m_mix, a_mix = fit_log_slope(lam2_mix, dvar_mix, xmin=lam2_min, xmax=lam2_max)
    if np.isfinite(m_mix):
        lam2_fit = np.array([lam2_min, lam2_max])
        dvar_fit = 10**(a_mix + m_mix * np.log10(lam2_fit))
        ax2.loglog(lam2_fit, dvar_fit, '--', linewidth=2, color='red',
                  label=f'slope = {m_mix:.2f} (Œª¬≤‚àà[0.3,10])')
        # Mark the fitting range
        ax2.axvspan(lam2_min, lam2_max, alpha=0.1, color='red', label='fit range')
    
    ax2.set_xlabel('$\\lambda^2$ (arb. units)')
    ax2.set_ylabel('$\\langle |dP/d\\lambda^2|^2 \\rangle$ (arb. units)')
    ax2.set_title('Derivative Measure: Mixed Case')
    ax2.grid(True, which='both', alpha=0.2)
    ax2.legend()
    
    plt.tight_layout()
    
    # Print analysis results
    print(f"\nDerivative measure slope analysis (Œª¬≤ ‚àà [0.3, 10]):")
    print(f"  Separated case slope: {m_sep:.2f}")
    print(f"  Mixed case slope:     {m_mix:.2f}")
    print(f"  Difference:           {abs(m_mix - m_sep):.2f}")
    
    # Interpretation
    print(f"\nInterpretation:")
    if np.isfinite(m_sep) and np.isfinite(m_mix):
        if abs(m_sep) < 0.1:
            print("  ‚Üí Separated case: nearly flat (expected for external screen)")
        if abs(m_mix) > 0.5:
            print("  ‚Üí Mixed case: significant slope (shows Œª-dependence)")
        print("  ‚Üí LP16 theory: derivative measure should show different behavior")
        print("    between separated (flat) and mixed (sloped) cases")
    
    return m_sep, m_mix

# -----------------------------
# Runner / demo
# -----------------------------

def run(h5_path: str, force_random_dominated: bool = False, decorrelate: bool = False, beam_sigma_px: float = 0.0, force_sequential: bool = False):
    """
    Run full LP16 PFA + derivative analysis with both one-point and two-point statistics.
    
    Args:
        h5_path: path to HDF5 file with MHD fields
        force_random_dominated: if True, subtract mean B_parallel to force œÉ_œÜ >> |œÜÃÑ|
    """
    keys = FieldKeys()
    cfg = PFAConfig()

    print("\n" + "="*70)
    print("LP16 PFA + DERIVATIVE ANALYSIS")
    print("="*70)

    Bx, By, Bz, ne = load_fields(h5_path, keys)
    Pi = polarized_emissivity(Bx, By, gamma=cfg.gamma)
    
    # Choose B_parallel based on cfg.los_axis
    if cfg.los_axis == 0:
        Bpar = Bz
    elif cfg.los_axis == 1:
        Bpar = By
    else:
        Bpar = Bx
    
    # Option to force random-dominated regime
    if force_random_dominated:
        print("\n‚ö† FORCING RANDOM-DOMINATED REGIME (subtracting mean B_‚à•)")
        Bpar = Bpar - Bpar.mean()
    
    phi = faraday_density(ne, Bpar, C=cfg.faraday_const)
    # Diagnose regime on the *native* œÜ (so r_i is physical)
    phi_info = compute_faraday_regime(phi, verbose=True)
    
    if decorrelate:
        print("‚öô  Decorrelating œÜ from Pi along LOS (roll; preserves r_i)")
        phi = decorrelate_phi_along_z(phi, cfg.los_axis, seed=42, mode="roll")
    LOS_depth = Pi.shape[cfg.los_axis]
    
    # Choose optimal wavelengths for analysis
    lam_long = choose_lambda_for_regime(phi_info, LOS_depth, target_regime="long")
    lam_short = choose_lambda_for_regime(phi_info, LOS_depth, target_regime="short")
    
    print(f"\nOptimal wavelengths:")
    print(f"  Short-Œª regime: Œª ‚âà {lam_short:.2f}")
    print(f"  Long-Œª regime:  Œª ‚âà {lam_long:.2f}")
    
    L_eff_long = effective_faraday_depth(lam_long, phi_info)
    L_eff_short = effective_faraday_depth(lam_short, phi_info)
    print(f"\nEffective Faraday depths:")
    print(f"  At Œª={lam_short:.2f}: L_eff ‚âà {L_eff_short:.1f} px  (vs LOS depth = {LOS_depth} px)")
    print(f"  At Œª={lam_long:.2f}: L_eff ‚âà {L_eff_long:.1f} px  (vs RM corr length ‚âà {phi_info['r_i']:.1f} px)")

    # Build a Œª-grid that spans short‚Üítransition‚Üílong regimes
    lam_mid = 1.0 / np.sqrt(max(abs(phi_info['phi_mean']), phi_info['phi_std']) * phi_info['r_i'])
    lam_min = max(lam_mid / 8.0, 1e-3)
    lam_max = lam_mid * 8.0
    cfg.lam_grid = tuple(np.sqrt(np.geomspace(lam_min**2, lam_max**2, 100)))

    # Use multiprocessing for faster computation (reduced for memory efficiency)
    n_processes = min(4, cpu_count())
    print(f"\nUsing {n_processes} parallel processes for computation")
    
    # Fallback: disable multiprocessing if memory issues occur
    use_multiprocessing = True
    if force_sequential:
        print("‚ö† Sequential computation forced by user")
        use_multiprocessing = False
    elif len(cfg.lam_grid) > 50:  # Large grid might cause memory issues
        print("‚ö† Large Œª-grid detected, using sequential computation to avoid memory issues")
        use_multiprocessing = False

    # ========================================
    # Part 1: One-point statistics (variance vs Œª¬≤)
    # ========================================
    print("\n" + "="*70)
    print("PART 1: One-point statistics (PFA variance vs Œª¬≤)")
    print("="*70)
    print("Expected: PFA and derivative curves have SAME Œª-shape, differ by constant")
    
    # PFA
    if use_multiprocessing:
        lam2_sep, var_sep = pfa_curve_separated(Pi, phi, cfg, n_processes=n_processes)
        lam2_mix, var_mix = pfa_curve_mixed(Pi, phi, cfg, n_processes=n_processes)
        # Derivative measure
        lam2_dsep, dvar_sep = pfa_curve_derivative_separated(Pi, phi, cfg, n_processes=n_processes)
        lam2_dmix, dvar_mix = pfa_curve_derivative_mixed(Pi, phi, cfg, n_processes=n_processes)
    else:
        # Sequential computation to avoid memory issues
        print("Computing PFA curves sequentially...")
        lam2_sep, var_sep = pfa_curve_separated(Pi, phi, cfg, n_processes=1)
        lam2_mix, var_mix = pfa_curve_mixed(Pi, phi, cfg, n_processes=1)
        # Derivative measure
        lam2_dsep, dvar_sep = pfa_curve_derivative_separated(Pi, phi, cfg, n_processes=1)
        lam2_dmix, dvar_mix = pfa_curve_derivative_mixed(Pi, phi, cfg, n_processes=1)

    # Combined plot of one-point statistics
    plot_combined_pfa_and_derivative(lam2_sep, [var_sep, var_mix], 
                                    lam2_dsep, [dvar_sep, dvar_mix],
                                    ["Separated", "Mixed"], cfg)
    suffix = "_random" if force_random_dominated else ""
    plt.savefig(f"lp2016_outputs/pfa_and_derivative_variance{suffix}.png", dpi=300)
    
    # ========================================
    # Part 1.5: Derivative slope analysis in specific Œª¬≤ range
    # ========================================
    print("\n" + "="*70)
    print("PART 1.5: Derivative measure slope analysis (Œª¬≤ ‚àà [0.3, 10])")
    print("="*70)
    
    # Analyze derivative measure slopes in the specified range
    m_der_sep, m_der_mix = plot_derivative_slope_analysis(lam2_dsep, dvar_sep, 
                                                          lam2_dmix, dvar_mix, cfg)
    plt.savefig(f"lp2016_outputs/derivative_slope_analysis{suffix}.png", dpi=300)

    # ========================================
    # Part 2: Two-point statistics (spatial PSA) at fixed Œª
    # ========================================
    print("\n" + "="*70)
    print("PART 2: Two-point (spatial) statistics at fixed Œª")
    print("="*70)
    print("This is what LP16 ¬ß6 actually analyzes!")
    print("\nExpected behavior by regime:")
    print("  ‚Ä¢ Random-dominated + long-Œª: derivative PSA better reveals turbulence slope")
    print("  ‚Ä¢ Mean-dominated + long-Œª: variance already shows slope ‚àù Œª^{-(2+2m)}")
    
    # Analyze at the optimal long-Œª value
    lam_test = lam_long
    print(f"\nComputing spatial PSA at Œª = {lam_test:.2f} (long-Œª regime)...")
    print(f"  L_eff/L ‚âà {L_eff_long/LOS_depth:.3f} (<<1 = strong rotation)")
    print(f"  L_eff/r_i ‚âà {L_eff_long/phi_info['r_i']:.3f}")
    
    # Mixed case
    print("\n  Computing MIXED case...")
    P_map_mix = P_map_mixed(Pi, phi, lam_test, cfg)
    dP_map_mix = dP_map_mixed(Pi, phi, lam_test, cfg)
    
    k_P,  Ek_P  = psa_of_map(P_map_mix,  ring_bins=48, pad=1, apodize=True,
                             k_min=6.0, min_counts_per_ring=10, beam_sigma_px=0.0)
    k_dP, Ek_dP = psa_of_map(dP_map_mix, ring_bins=48, pad=1, apodize=True,
                             k_min=6.0, min_counts_per_ring=10, beam_sigma_px=0.0)
    print(f"  MIXED: usable PSA bins ‚Üí P: {len(k_P)}, dP: {len(k_dP)}")
    
    m_P, m_dP = plot_spatial_psa_comparison(k_P, Ek_P, k_dP, Ek_dP, lam_test, case="Mixed")
    plt.savefig(f"lp2016_outputs/psa_P_vs_dP_spatial{suffix}.png", dpi=300)
    
    # Separated case
    print("\n  Computing SEPARATED case...")
    P_map_sep = P_map_separated(Pi, phi, lam_test, cfg)
    dP_map_sep = dP_map_separated(Pi, phi, lam_test, cfg)
    k_P_sep,  Ek_P_sep  = psa_of_map(P_map_sep,  ring_bins=48, pad=1, apodize=True,
                                     k_min=6.0, min_counts_per_ring=10, beam_sigma_px=0.0)
    k_dP_sep, Ek_dP_sep = psa_of_map(dP_map_sep, ring_bins=48, pad=1, apodize=True,
                                     k_min=6.0, min_counts_per_ring=10, beam_sigma_px=0.0)
    print(f"  SEPARATED: usable PSA bins ‚Üí P: {len(k_P_sep)}, dP: {len(k_dP_sep)}")
    
    m_P_sep, m_dP_sep = plot_spatial_psa_comparison(k_P_sep, Ek_P_sep, 
                                                     k_dP_sep, Ek_dP_sep, 
                                                     lam_test, case="Separated")
    plt.savefig(f"lp2016_outputs/psa_P_vs_dP_spatial_separated{suffix}.png", dpi=300)
    
    # ========================================
    # Summary
    # ========================================
    print("\n" + "="*70)
    print("SUMMARY")
    print("="*70)
    print(f"\nRegime: {phi_info['regime'].upper()}")
    print(f"  |œÜÃÑ|/œÉ_œÜ = {phi_info['ratio']:.2f}")
    
    print("\n1. One-point statistics (variance vs Œª¬≤):")
    print("   ‚úì PFA and derivative curves differ only by constant factor")
    print("   ‚úì Both show same Œª-dependence (Gaussian Œ¶ factorization)")
    print("   ‚Üí This is EXPECTED from LP16 theory")
    
    print(f"\n1.5. Derivative slope analysis (Œª¬≤ ‚àà [0.3, 10]):")
    print(f"   Separated case slope: {m_der_sep:.2f}")
    print(f"   Mixed case slope:     {m_der_mix:.2f}")
    print(f"   Difference:           {abs(m_der_mix - m_der_sep):.2f}")
    
    print(f"\n2. Two-point statistics (spatial PSA at Œª={lam_test:.2f}):")
    print(f"   Mixed case:")
    print(f"     PSA(P) slope:       {m_P:.2f} (fitted k‚àà[15,80])")
    print(f"     PSA(dP/dŒª¬≤) slope:  {m_dP:.2f} (fitted k‚àà[15,80])")
    print(f"     Difference:         {abs(m_dP - m_P):.2f}")
    print(f"   Separated case:")
    print(f"     PSA(P) slope:       {m_P_sep:.2f} (fitted k‚àà[15,80])")
    print(f"     PSA(dP/dŒª¬≤) slope:  {m_dP_sep:.2f} (fitted k‚àà[15,80])")
    print(f"     Difference:         {abs(m_dP_sep - m_P_sep):.2f}")
    
    print("\n3. Interpretation:")
    if phi_info['regime'] == "random-dominated":
        print("   ‚úì Random-dominated regime: derivative PSA should help recover m")
        print("   ‚úì PFA variance ‚Üí universal Œª‚Åª¬≤ masks turbulence")
        print("   ‚úì Derivative spatial PSA ‚Üí better reveals turbulence slope")
    else:
        print("   ‚Ä¢ Mean-dominated regime: variance already reveals slope")
        print("   ‚Ä¢ Expect ‚ü®|P|¬≤‚ü© ‚àù Œª^{-(2+2m)} from transverse field")
        print("   ‚Ä¢ Derivative PSA less critical (variance works well)")
        print("\n   üí° TIP: Run with force_random_dominated=True to see derivative advantage")

    # Optional quick re-run at the same Œª but with random-dominated RM (for comparison)
    if not force_random_dominated:
        print("\n--- Quick comparison: random-dominated at same Œª (diagnostic) ---")
        Bpar_rd = Bpar - Bpar.mean()
        phi_rd  = faraday_density(ne, Bpar_rd, C=cfg.faraday_const)
        P_map_rd  = P_map_mixed(Pi, phi_rd, lam_test, cfg)
        dP_map_rd = dP_map_mixed(Pi, phi_rd, lam_test, cfg)
        kP_rd, EP_rd   = psa_of_map(P_map_rd,  ring_bins=48, pad=1, apodize=True,
                                    k_min=6.0, min_counts_per_ring=10, beam_sigma_px=0.0)
        kDP_rd, EDP_rd = psa_of_map(dP_map_rd, ring_bins=48, pad=1, apodize=True,
                                    k_min=6.0, min_counts_per_ring=10, beam_sigma_px=0.0)
        # Use auto-fit window for random-dominated comparison
        def pick_fit_window(k, E):
            k = np.asarray(k); E = np.asarray(E)
            m = np.isfinite(k) & np.isfinite(E) & (k > 0) & (E > 0)
            k, E = k[m], E[m]
            if k.size < 12:
                return 12.0, 64.0
            logk, logE = np.log10(k), np.log10(E)
            best = (np.inf, 12.0, 64.0)
            for p in (30, 40, 50, 60, 70):
                kmid = 10**np.percentile(logk, p)
                for span in (2.0, 2.5, 3.0):
                    lo = kmid / (10**(span/6))
                    hi = kmid * (10**(span/6))
                    sel = (k >= lo) & (k <= hi)
                    if sel.sum() >= 10:
                        s = np.diff(logE[sel]) / np.diff(logk[sel])
                        curv = np.nanstd(s)
                        if curv < best[0]:
                            best = (curv, lo, hi)
            return best[1], best[2]
        k_min_rd, k_max_rd = pick_fit_window(kP_rd, EP_rd)
        m_P_rd, a_P_rd = fit_log_slope(kP_rd, EP_rd, xmin=k_min_rd, xmax=k_max_rd)
        m_dP_rd, a_dP_rd = fit_log_slope(kDP_rd, EDP_rd, xmin=k_min_rd, xmax=k_max_rd)
        print(f"  Random-dominated PSA slopes at Œª={lam_test:.2f}:")
        print(f"    PSA(P) slope:       {m_P_rd:.2f}")
        print(f"    PSA(dP/dŒª¬≤) slope:  {m_dP_rd:.2f}")
        print(f"    Difference:         {abs(m_dP_rd - m_P_rd):.2f}")
        print("    ‚Üí Should show clearer derivative advantage in random-dominated regime")
    
    print("\n" + "="*70)

    #plt.show()


if __name__ == "__main__":
    import sys
    
    print("sys.argv:", sys.argv)
    # Parse command line arguments
    force_random = "--random" in sys.argv or "-r" in sys.argv
    both_regimes = "--both" in sys.argv or "-b" in sys.argv
    decorrelate = "--decorrelate" in sys.argv or "-d" in sys.argv
    sequential = "--sequential" in sys.argv or "-s" in sys.argv
    beam = 0.0
    for a in sys.argv:
        if a.startswith("--beam="):
            try: beam = float(a.split("=")[1])
            except: pass
    
    if both_regimes:
        print("\n" + "="*70)
        print("RUNNING ANALYSIS FOR BOTH REGIMES")
        print("="*70)
        
        print("\n\n" + "#"*70)
        print("# ANALYSIS 1: NATURAL REGIME (as-is)")
        print("#"*70)
        run(H5_PATH, force_random_dominated=False, decorrelate=decorrelate, beam_sigma_px=beam, force_sequential=sequential)
        
        print("\n\n" + "#"*70)
        print("# ANALYSIS 2: FORCED RANDOM-DOMINATED REGIME")
        print("#"*70)
        run(H5_PATH, force_random_dominated=True, decorrelate=decorrelate, beam_sigma_px=beam, force_sequential=sequential)
        
        print("\n\n" + "="*70)
        print("DONE! Check lp2016_outputs/ for comparison:")
        print("  ‚Ä¢ *_variance.png vs *_variance_random.png")
        print("  ‚Ä¢ *_spatial.png vs *_spatial_random.png")
        print("="*70)
    else:
        run(H5_PATH, force_random_dominated=force_random, decorrelate=decorrelate, beam_sigma_px=beam, force_sequential=sequential)
    
    # Usage help
    if "--help" in sys.argv or "-h" in sys.argv:
        print("\nUsage:")
        print("  python pfa_and_derivative_lp_16_like.py         # Natural regime")
        print("  python pfa_and_derivative_lp_16_like.py -r      # Force random-dominated")
        print("  python pfa_and_derivative_lp_16_like.py -d      # Decorrelate Pi-Œ¶")
        print("  python pfa_and_derivative_lp_16_like.py -r -d   # Random + decorrelate")
        print("  python pfa_and_derivative_lp_16_like.py --beam=1.5  # Add beam smoothing")
        print("  python pfa_and_derivative_lp_16_like.py -s      # Force sequential (avoid memory issues)")
        print("  python pfa_and_derivative_lp_16_like.py -b     # Run both for comparison")
